// Test the FunctionAdapter on a simple quadratic with a linear constraint

// See Platt and Barr "Constrained Differential Optimization", (1988)

import optimize3

fn func(x, y) {
    return x^2 + y^2
}

fn g(x, y) {
    return x + y - 1
}

class LagrangeMultiplierAdapter is OptimizationAdapter {
  init(adapter) {
    self.adapt = adapter
    self.nconstraints = adapter.countconstraints()
    self.lambda = Matrix(self.nconstraints)
    self.penalty = 100
  }

  set(x) { // Get parameters, stripping off lagrange multipliers
    var n = x.count()
    var nvars = n - self.nconstraints
    self.adapt.set(x[0...nvars,0])
    self.lambda = x[nvars...n,0] // Store lagrange multipliers on this adapter
  }

  get() { // Get parameters, joining real parameters with lagrange multipliers
    var x = self.adapt.get()
    return Matrix([[x], [self.lambda]]) 
  }

  value() { // Lagrangian = f - lambda_i g_i
    var f = self.adapt.value() 
    var g = Matrix(self.adapt.constraintvalue())

    return f + self.lambda.inner(g)  
  }

  gradient() { // Gradient of Lagrangian is [ df - lambda_i dg_i , - g] 
    var grad = self.adapt.gradient()

    var consval = Matrix(self.adapt.constraintvalue())
    var consgrad = self.adapt.constraintgradient()

    for (c,k in consgrad) grad += self.lambda[k]*c

    return Matrix([[grad],[-consval]]) // Note constraint gradient is reversed 
  } 
}

var start = Matrix([0,0])

var adapt = FunctionAdapter(func, start=start, constraints=[g])

var ladapt = LagrangeMultiplierAdapter(adapt)

var control = GradientDescentController(ladapt)
//var control = LineSearchController(ladapt)
// Can't use linesearches on this because we're finding a saddle point

control.optimize(200)

print "Value:"
print ladapt.value()
print "Soln:"
print ladapt.get()
print "Grad:"
print ladapt.gradient()
 