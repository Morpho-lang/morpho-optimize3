// Thomson problem demonstrating local equality constraints

import meshtools 
import optimize3
import functionals
import plot 

var Np = 200 // Number of particles

var L = 100

// Create the mesh, which consists of Np random points each representing
// a charge on the unit sphere.
var build = MeshBuilder()
for (i in 1..Np) {
  var x = Matrix([2*random()-1, 2*random()-1, 2*random()-1])
  x/=x.norm() // Project onto unit sphere
  build.addvertex(x)
}
var mesh = build.build() // Tell the MeshBuilder to build the mesh

// Specify the problem
var problem = OptimizationProblem(mesh)

var lv = PairwisePotential(fn (r) 1/r, fn (r) -1/r^2)
problem.addenergy(lv)

var lback = ScalarPotential(fn (x,y,z) x^2+y^2+z^2)
problem.addenergy(lback, prefactor=L)

var lsph = ScalarPotential(fn (x,y,z) x^2+y^2+z^2-1)
problem.addlocalconstraint(lsph)

var adapt = MeshAdapter(problem, mesh) 


var _OptUnConsErr = Error("OpUncons", "Problem appears to be unconstrained.")
var _OptLnSrchDrn = Error("OptLnSrchDrn", "Linesearch encountered an upward direction.")


class XLineSearchController is LineSearchController {
  setexpecteddescent(df) {
    self.df = df
    if (self.df>0) self.warning(_OptLnSrchDrn)
  }

  expecteddescent() { // Predict expected descent
    return self.df
  }
}

class L1PenaltyAdapter is DelegateAdapter {
  init(adapter, penalty=1) {
    self.adapter = adapter
    self.penalty = penalty
  }

  constraintvector() {
    var v = self.adapter.constraintvalue()

    var l = []
    for (c in v) {
      if (ismatrix(c)) for (x in c) l.append(x)
      else l.append(c)
    }

    return Matrix(l)
  }

  value() { //  f + lambda_i c_i
    var f = self.adapter.value() 
    var c = self.constraintvector()

    return f + self.penalty*c.norm(1)
  }

  directionalderivative(d) {
    var df = self.adapter.gradient() 
    var c = self.constraintvector()

    return df.inner(d) - self.penalty*c.norm(1)
  }
}

class LagrangeMultiplierAdapter is OptimizationAdapter {
  init(adapter) {
    self.adapter = adapter
    self.nconstraints = self.constraintvector().count() 
    self.lambda = Matrix(self.nconstraints)
  }

  set(x) { // Get parameters, stripping off lagrange multipliers
    var n = x.count()
    var nvars = n - self.nconstraints
    self.adapter.set(x[0...nvars,0])
    self.lambda = x[nvars...n,0] // Store lagrange multipliers on this adapter
  }

  get() { // Get parameters, joining real parameters with lagrange multipliers
    var x = self.adapter.get()
    return Matrix([[x], [self.lambda]]) 
  }

  setlagrangemultipliers(lambda) {
    self.lambda = lambda
  }

  lagrangemultipliers() {
    return self.lambda 
  }

  _checkineq(c) {
    var nc = self.adapter.countconstraints()
    for (k in nc[0]...nc[0]+nc[1]) {
      if (c[k]>0) c[k]=0
    }
  }

  constraintvector() {
    var v = self.adapter.constraintvalue()

    var l = []
    for (c in v) {
      if (ismatrix(c)) for (x in c) l.append(x)
      else l.append(c)
    }

    return Matrix(l)
  }

  value() { // Lagrangian = f + lambda_i c_i
    var f = self.adapter.value() 
    var c = self.constraintvector()

    return f + self.lambda.inner(c)  
  }

  vargradient() { // Gradient of lagrangian wrt original degrees of freedom
    var grad = self.adapter.gradient()
    var cgrad = self.adapter.constraintgradient()
    var xcg = Matrix([cgrad])

    grad += xcg*self.lambda 
    //@
    //for (cg,k in cgrad) grad += self.lambda[k]*cg

    return grad 
  }

  gradient() { // Gradient of Lagrangian is [ df + lambda_i dc_i , c_i] 
    var c = self.constraintvector()

    return Matrix([[self.vargradient()],[c]]) 
  } 

  hessian() { // Hessian of Lagrangian is the kkt matrix 
    var h = self.adapter.hessian()    

    var chess = self.adapter.constrainthessian() 
    for (ch,k in chess) h += self.lambda[k]*ch

    var cg = self.adapter.constraintgradient()
    var C = Matrix([cg])

    return Matrix([[h, C],[C.transpose(), 0]])
  }
}

class XLBFGSController is InvBFGSController {
  init(adapter, quiet=false, verbosity=nil, maxhistorylength=10) {
    super.init(adapter, quiet=quiet, verbosity=verbosity)
    self.maxhistorylength = maxhistorylength
  }

  _popfirst(lst) { // Pops the first element of a list
    var a = lst.roll(-1)
    a.pop() 
    return a 
  }

  _hmul(p) { // Compute the action of our approximation to the inv H on a vector or matrix 
    var q = p

    var n = self._storage.count() 
    if (n==0) return q
    
    var alpha[n]

    // Project off all components of p that lie in the history directions
    for (k in n-1..0:-1) { // Start with most recent
      var z = self._storage[k]
      var sk = z[0], yk = z[1], rho = 1/z[2]

      alpha[k] = rho * sk.transpose()*q //sk.inner(q)
      q-=yk*alpha[k]
    } 

    var zl = self._storage[-1] // Last 
    var r = (zl[2]/zl[3])*q // (sk.yk)/(yk.yk)

    for (z, k in self._storage) { // Work forwards
      var sk = z[0], yk = z[1], rho = 1/z[2]

      var beta = rho * yk.transpose()*r// rho*yk.inner(r)
      r+=sk*(alpha[k]-beta)
    }

    return r 
  }

  start() {
    self._storage = []
  }

  direction() {
    return -self._hmul(self.gradient())
  }

  update(sk, yk) {
    self._storage.append((sk, yk, sk.inner(yk), yk.inner(yk)))

    if (self._storage.count()>self.maxhistorylength) self._storage = self._popfirst(self._storage)
  }

  next() { // Update our estimate of the inverse Hessian
    var sk = self.get() - self._ox
    var yk = self.gradient() - self._ogradient

    self.update(sk, yk) 
  }
}

class XBFGSController is NewtonController {
  hessian() { // Return our estimate of the hessian
    return self.B
  }

  start() {
    self.B = IdentityMatrix(self.get().count())
  }

  begin() {
    super.begin() 
    self._ox = self.get() 
    self._ogradient = self.gradient() 
  }

  update(sk, yk) {
    var u = self.B*sk 
    var yksk = yk.inner(sk)
    var sku = sk.inner(u)
    if (yksk==0 || sku==0) { self.start(); return }

    self.B += yk*yk.transpose()/yksk - u*u.transpose()/sku
  }

  next() {
    var sk = self.get() - self._ox
    var yk = self.gradient() - self._ogradient

    self.update(sk,yk)
  }
}

class XInvBFGSController is BFGSController {
  start() {
    super.start() 
    self.I = IdentityMatrix(self.get().count())
  }

  direction() {
    var g = self.gradient()
    return -self.B*g 
  }

  update(sk, yk) {
    var v = yk.inner(sk)
    if (v==0) { self.B=self.I; return }
    var U = self.I - yk*sk.transpose()/v

    self.B = U.transpose()*self.B*U + sk*sk.transpose()/v
  }

  next() {
    var sk = self.get() - self._ox
    var yk = self.gradient() - self._ogradient

    self.update(sk, yk)
  }
}

class SQPController is OptimizationController {
  gradient() {
    return self.ladapter.gradient()
  }

  kkt() {
    return self.ladapter.hessian() 
  }

  start() {
    self.ladapter = LagrangeMultiplierAdapter(self.adapter)

    self.bfgs = XBFGSController(self.adapter)
    self.bfgs.start() 

    self.lbfgs = XLBFGSController(self.adapter)
    self.lbfgs.start() 

    self.ibfgs = XInvBFGSController(self.adapter)
    self.ibfgs.start() 
  }

  begin() {
    self._ox = self.get() 
    //self._ogradient = self.ladapter.vargradient() 
    self._ogradient = self.adapter.gradient()
  }

  solve() {
    var rhs = self.gradient() 
    
    var H = self.bfgs.B

    var cg = self.adapter.constraintgradient()
    var C = Matrix([cg])

    var kkt = Matrix([ [H,             C ],
                       [C.transpose(), 0] ])

    return -rhs/kkt
  }

  Xsolve() {
    var rhs = self.gradient() 
    var kkt = self.kkt() 
    return -rhs/kkt
  }

  Ysolve() {
    var g = self.ladapter.vargradient() 
    var c = self.ladapter.constraintvector()
    var cg = self.adapter.constraintgradient()
    var C = Matrix([cg])

    var Ct = C.transpose()

    var Hg = self.lbfgs._hmul(g)
    var HC = self.lbfgs._hmul(C)

    var S = -Ct*HC
    var Sinvc = c/S
    var SinvCt = (Ct*Hg)/S

    var HCSinvCt = self.lbfgs._hmul(C*SinvCt)
    var HCCSc = self.lbfgs._hmul(C*Sinvc)

    return -Matrix([ [Hg + HCSinvCt - HCCSc],
                     [- SinvCt + Sinvc] ])
  }

  Zsolve() {
    var g = self.ladapter.vargradient() 
    var c = self.ladapter.constraintvector()
    var cg = self.adapter.constraintgradient()
    var C = Matrix([cg])
    var Ct = C.transpose()

    var Hinv = self.ibfgs.B

    var S = -Ct*Hinv*C
    
    var Hg = Hinv*g 
    var Sinvc = c/S
    var SinvCHg = Ct*Hg/S

    var HCSinvCHG = Hinv*C*SinvCHg

    return -Matrix([ [Hg + HCSinvCHG - Hinv*C*Sinvc],
                     [- SinvCHg + Sinvc] ])
  }

  linesearch(dirn) {
    var padapt = L1PenaltyAdapter(self.adapter, penalty=10)

    var x0 = self.adapter.get()

    var ndof = x0.count() 
    var xdirn = dirn[0...ndof]

    var ls = XLineSearchController(padapt)
    ls.setdirection(xdirn)
    var df = padapt.directionalderivative(xdirn) 

    ls.setexpecteddescent(df)
    ls.step()

    self.adapter.set(x0)

    return ls.stepsize
  }

  step() {
    var x0 = self.ladapter.get()
    var ndof = self.adapter.get().count()

    var d = self.Ysolve()
    var alpha = self.linesearch(d)

    var dl = d.clone() 
    for (i in 0...ndof) dl[i]=0
    self.ladapter.set(x0 + alpha*dl)
    self._ogradient = self.ladapter.vargradient() 

    self.ladapter.set(x0 + alpha*d)

    self.stepsize = alpha 
  }

  reportstepsize() { return "stepsize=${self.stepsize}" }

  next() {
    // Crucially, yk = grad L(x+alpha d, lambda k+1)-grad L(x, lambda k+1)
    // i.e. holding lambda constant
    var sk = self.get() - self._ox
    //var yk = self.adapter.gradient() - self._ogradient
    var yk = self.ladapter.vargradient() - self._ogradient

    self.bfgs.update(sk, yk)
    self.lbfgs.update(sk, yk)
    self.ibfgs.update(sk, yk)
  }
}

var control = SQPController(adapt)
control.optimize(1000)

print adapt.value() - L*lback.total(mesh)

var g = Graphics()
for (i in 0...mesh.count()) {
  g.display(Sphere(mesh.vertexposition(i),1/sqrt(Np)))
}
Show(g) 