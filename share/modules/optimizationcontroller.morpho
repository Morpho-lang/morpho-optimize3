/* ************************************************************
 * OptimizationController
 * ************************************************************ */ 

/* --------------------------
 * Errors
 * -------------------------- */ 

var _OptInf = Error("OptInf", "Optimizer encountered an infinite or non-numerical value.")
var _OptMaxErr = Error("OptMaxIter", "Maximum iterations exceeded: present solution does not meet convergence criteria.")
var _OptUnknwnVrbsty = Error("OptUnknwnVrbsty", "Unknown verbosity setting: use 'silent', 'quiet', 'normal', or 'verbose'.")

var _OptLnSrchDrn = Error("OptLnSrchDrn", "Linesearch encountered an upward direction.")
var _OptLnSrchStpsz = Error("OptLnSrchStpsz", "Linesearch stepsize has tended to zero.")
var _OptLnSrchZm = Error("OptLnSrchZm", "Maximum iterations exceeded in linesearch interval identification.")

var _OptNoGrad = Error("OptNoGrad", "Method requires a gradient.")
var _OptNoHess = Error("OptNoHess", "Method requires a hessian.")

/* --------------------------
 * Reporting levels 
 * -------------------------- */ 

var _OptSilent  = 0 // Only errors
var _OptQuiet   = 1 // Warnings and errors only
var _OptNormal  = 2 // Normal output including 
var _OptVerbose = 3 // Additional debugging information

var _OptVerbosity = { "silent" : _OptSilent,
                      "quiet" : _OptQuiet,
                      "normal" : _OptNormal, 
                      "verbose" : _OptVerbose }

/* --------------------------
 * OptimizationController 
 * -------------------------- */ 

/* OptimizationController is a base class that implements an 
   optimization algorithm or a subcomponent that works by invoking
   the interface of an OptimizationAdapter. */ 

class OptimizationController is DelegateAdapter {
  init(adapter, quiet=false, verbosity=nil) { 
    super.init(adapter)
    
    // Fit a ProxyAdapter over the regular adapter if not present to avoid repeated calculations
    if (adapter.clss()!=ProxyAdapter) self.adapter = ProxyAdapter(adapter)
    
    // Control verbosity 
    self.verbosity = _OptNormal
    if (quiet) self.verbosity = _OptQuiet
    if (verbosity) {
      if (_OptVerbosity.contains(verbosity)) self.verbosity = _OptVerbosity[verbosity]
      else if (isint(verbosity)) self.verbosity = verbosity
      else _OptUnknwnVrbsty.throw() 
    }

    // Tolerances
    self.gradtol = 1e-6 // Convergence criterion based on norm of the gradient
    self.etol = 1e-8 // Convergence criterion based on change in value of objective function
    self.ctol = 1e-10 // Constraint tolerance
    self.maxconsiterations = 100 // Maximum number of iterations to satisfy constraints

    self._valuehistory=[] // History of objective function values 
  } 

  /* Convenience methods to calculate quantities */

  value() { // Value of the objective function
    var v = self.adapter.value()
    if (isinf(v) || isnan(v)) _OptInf.throw() 
    return v
  }

  constraintvalue() { // Value of constraint functions as a vector
    var a = super.constraintvalue()
    return Matrix(a)
  }

  constraintgradient() { // Gradient of constraint functions as a matrix
    var g = super.constraintgradient()
    return Matrix([g])
  }

  /* Verbosity */

  checkverbosity(level) { // Checks if output should be printed at a given level
    return (self.verbosity>=level)
  }

  warning(error) { // Generate a warning 
    if (self.checkverbosity(_OptQuiet)) error.warning() 
  }

  /* Convergence check */

  hasconverged() {
    // Convergence in value  |delta e|/|e| < etol or |delta e| < etol if e is < etol
    if (self._valuehistory && self._valuehistory.count()>1) {
      var f1=self._valuehistory[-1], f2=self._valuehistory[-2]

      // Compute relative change in value, or use absolute value if close to zero
      var de = abs(f1-f2), ee=(abs(f1)+abs(f2))/2, tol = self.etol 
      if (abs(f1)>tol) tol*=ee 
      if (de < tol) return true 
    } 

    return false 
  }

  /* Reporting */

  reportoptcond() { // Report value of optimality condition
    var g = self.gradient()
    if (!g) return ""
    return "grad=|${g.norm()}|"
  }

  reportstepsize() { return "" }

  report(iter) { // Reporting
    if (!self.checkverbosity(_OptNormal)) return 

    var f = self.value() 

    print "Iteration ${iter.format("%2i")}: ${f} ${self.reportoptcond()} ${self.reportstepsize()}" 
  }

  /* Optimization sequence */

  start() { }           // Initialization at the beginning of an optimization
  begin() { }           // Calculate initial information for the iteration
  step() { }            // Perform optimization step 
  next() { }            // Calculate updated information

  record() {            // Record information about the iteration
    self._valuehistory.append(self.value())
  }

  iterate() {  // Optimization iteration
    self.begin()  
    self.step()
    self.next()
  }

  optimize(nsteps) { // Optimization loop
    self.start()
    self.record() 
    for (i in 1..nsteps) {
      if (self.hasconverged()) return 
      self.iterate() 
      self.report(i)
      self.record()
    }

    self.warning(_OptMaxErr)
  }
}

/* ************************************************************
 * Line search methods
 * ************************************************************ */ 

/* -----------------------------------------
 * GradientDescentController
 * 
 * Simple gradient descent at fixed stepsize 
 * ----------------------------------------- */ 

class GradientDescentController is OptimizationController {
  init(adapter, quiet=false, verbosity=nil, stepsize=0.1) { 
    super.init(adapter, quiet=quiet, verbosity=verbosity)
    self.stepsize=stepsize
    self._direction=nil 
  }

  // Define a descent direction
  setdirection(d) { self._direction = d }
  direction() { return self._direction}

  searchdirection() { // Determine the search direction
    self.setdirection(-self.gradient())
  }

  begin() { self.searchdirection() }

  step() { // Take a step in the current search direction
    var x = self.get() 
    x+=self.stepsize*self.direction()
    self.set(x)
  }

  reportstepsize() { return "stepsize=${self.stepsize}" }
}

/* -----------------------------------------
 * LineSearchController
 * 
 * Armijo linesearches
 * ----------------------------------------- */ 

class LineSearchController is GradientDescentController {
  init(adapter, quiet=false, verbosity=nil, stepsize=0.1, alpha=0.2, beta=0.5) {
    super.init(adapter, quiet=quiet, verbosity=verbosity, stepsize=stepsize)
    self.maxsteps = 50 // Maximum steps for reducing stepsize
    self.alpha = alpha // } Coefficients 
    self.beta = beta   // }
  }

  expecteddescent() { // Predict expected descent
    var d = self.direction()
    var g = self.gradient() 

    self.df = g.inner(d)

    if (self.df>0) _OptLnSrchDrn.warning()
  }

  accept(t) { // Have we descended sufficiently?
    return self.value() < self._ovalue + self.alpha*t*self.df
  }

  stepwith(x0, t) { // Take a step of size t from x0 in the descent direction
    self.set(x0 + t*self.direction())
    return self.value()
  }

  step() {// Perform a single linesearch
    self.expecteddescent()
    var t=1
    var success=false
    self._ovalue = self.value() // Save value for comparison

    var x0 = self.get() 
    for (nsteps in 1..self.maxsteps) {
        self.stepwith(x0, t)
        success=self.accept(t)
        if (success) break
        t*=self.beta
    }
    if (!success) _OptLnSrchStpsz.warning()

    self.stepsize=t
  }
}

/* -----------------------------------------
 * WolfeLineSearchController
 * 
 * Linesearches that satisfy strong Wolfe 
 * conditions [Nocedal&Wright Chapter 3, p60]
 * ----------------------------------------- */ 

// See also SciPy implementation https://indrag49.github.io/Numerical-Optimization/line-search-descent-methods.html#selection-of-step-length
// See also an implementation at https://github.com/gjkennedy/ae6310/blob/master/Line%20Search%20Algorithms.ipynb

class WolfeLineSearchController is LineSearchController {
  init(adapter, quiet=false, verbosity=nil, stepsize=1, steplimit=2, c1=1e-3, c2=0.9) {
    super.init(adapter, quiet=quiet, verbosity=verbosity)
    self.maxsteps = 50 // Maximum steps for reducing stepsize
    self.stepsize = stepsize   // Initial stepsize
    self.steplimit = steplimit // Stepsize limit
    self.c1 = c1 // } Coefficients 
    self.c2 = c2 // }
    self.verbose = self.checkverbosity(_OptVerbose)
  }

  _interpolate(f0, df, alpha, fa) {
    return - df*alpha^2/(2*(fa-f0-df*alpha))
  }

  _zoom(alphalo0, alphahi0, x0, f0, df) {
    var alphalo = alphalo0, alphahi = alphahi0
    var flo = self.stepwith(x0, alphalo)

    for (_ in 1..self.maxsteps) {
      if (self.verbose) print "  Zoom iteration ${_}: (${alphalo},${alphahi}) [${flo}, ${self.stepwith(x0, alphahi)}]"
      var alpha = (alphalo+alphahi)/2 // Bisection

      var f = self.stepwith(x0, alpha)

      if (f>f0+self.c1*alpha*df || // Armijo condition
          f >= flo) {
            if (self.verbose) print "    Armijo test failed; reduce stepsize"
            alphahi = alpha 
      } else {
        self.gradient() 
        var dfalpha = self.gradient().inner(self.direction())

        if (self.verbose) print "    Testing curvature condition: ${abs(dfalpha)} <= ${self.c2*abs(df)}"
        if (abs(dfalpha) <= self.c2*abs(df)) { // Curvature condition
          if (self.verbose) print "    Curvature condition succeeded alpha=${alpha.format("%.16g")}"
          self.stepsize = alpha 
          return 
        } else if (dfalpha*(alphahi-alphalo)>=0) {
          if (self.verbose) print "    Swap intervals"
          alphahi = alphalo 
        }

        if (self.verbose) print "    Increase stepsize "
        alphalo = alpha  
        flo = f 
      }
    }
 
    _OptLnSrchZm.warning() 
  }

  step() { // Perform a single linesearch; Alg. 3.5 of N&D
    self.expecteddescent()

    var f0 = self.value()
    var df = self.df 
    var x0 = self.get() 

    var alpha=self.stepsize
    var oalpha=0
    var of = f0 

    for (_ in 1..self.maxsteps) {
      var f = self.stepwith(x0, alpha)
      if (self.verbose) print "  ls iteration ${_}: alpha=${alpha} f=${f}"

      if (f>f0+self.c1*alpha*df ||
          f>=of) { // Step is too long if Armijo test fails or the function simply increased
        if (self.verbose) print "  Armijo test failed; zooming on interval (${oalpha},${alpha})"
        return self._zoom(oalpha, alpha, x0, f0, df) 
      }
       
      var dfalpha = self.gradient().inner(self.direction())

      if (abs(dfalpha) <= self.c2*abs(df)) { // Curvature test
        if (self.verbose) print "  Curvature test succeeded alpha=${alpha.format("%.16g")}"
        self.stepsize = alpha // Success
        if (self.verbose) print self.adapter.get()
        return 
      }

      if (dfalpha>=0) { // Gradient is upward so reduce stepsize
        if (self.verbose) print "  Upward gradient detected; zooming on interval (${alpha},${oalpha})"
        return self._zoom(alpha, oalpha, x0, f0, df)
      }
      
      of = f 
      oalpha = alpha 

      //alpha = self._interpolate(f0, df, alpha, f)
      alpha = min(2*alpha, self.steplimit)
    }

    if (self.verbose) print "Too many iterations in step."
  }

  _interpolate(x0,x1,f0,f1,df0,df1) { // Cubic interpolation Nocedal p59
    var d1 = df0 + df1 - 3*(f1-f0)/(x1-x0)
    var s = d1^2-df0*df1
    if (s<0) return (x0+x1)/2
    var d2 = sign(x1-x0)*sqrt(s)

    return x1-(x1-x0)*(df1+d2-d1)/(df1-df0-2*d2) // Should check if outside interval
  }
}

/* ************************************************************
 * Newton and quasi-Newton methods
 * ************************************************************ */ 

/* -----------------------------------------
 * Newton's method
 * ----------------------------------------- */ 

class NewtonController is LineSearchController {
  searchdirection() {
    super.searchdirection()

    var H = self.hessian() 
    if (!H) _OptNoHess.throw("NewtonController can't use ${self.adapter.clss()} because it doesn't provide a hessian.") 

    var d = self.direction()/H // direction was already -grad from superclass
    self.setdirection(d)
  }
}
