/* ************************************************************
 * OptimizationController
 * ************************************************************ */ 

/* --------------------------
 * Errors
 * -------------------------- */ 

var _OptInf = Error("OptInf", "Optimizer encountered an infinite or non-numerical value.")
var _OptMaxErr = Error("OptMaxIter", "Maximum iterations exceeded: present solution does not meet convergence criteria.")
var _OptUnknwnVrbsty = Error("OptUnknwnVrbsty", "Unknown verbosity setting: use 'silent', 'quiet', 'normal', or 'verbose'.")

var _OptLnSrchDrn = Error("OptLnSrchDrn", "Linesearch encountered an upward direction.")
var _OptLnSrchStpsz = Error("OptLnSrchStpsz", "Linesearch stepsize has tended to zero.")

/* --------------------------
 * Reporting levels 
 * -------------------------- */ 

var _OptSilent  = 0 // Only errors
var _OptQuiet   = 1 // Warnings and errors only
var _OptNormal  = 2 // Normal output including 
var _OptVerbose = 3 // Additional debugging information

var _OptVerbosity = { "silent" : _OptSilent,
                      "quiet" : _OptQuiet,
                      "normal" : _OptNormal, 
                      "verbose" : _OptVerbose }

/* --------------------------
 * OptimizationController 
 * -------------------------- */ 

/* OptimizationController is a base class that implements an 
   optimization algorithm or a subcomponent that works by invoking
   the interface of an OptimizationAdapter. */ 

class OptimizationController is DelegateAdapter {
  init(adapter, quiet=false, verbosity=nil) { 
    super.init(adapter)
    
    // Fit a ProxyAdapter over the regular adapter if not present to avoid repeated calculations
    if (adapter.clss()!=ProxyAdapter) self.adapter = ProxyAdapter(adapter)
    
    // Control verbosity 
    self.verbosity = _OptNormal
    if (quiet) self.verbosity = _OptQuiet
    if (verbosity) {
      if (_OptVerbosity.contains(verbosity)) self.verbosity = _OptVerbosity[verbosity]
      else if (isint(verbosity)) self.verbosity = verbosity
      else _OptUnknwnVrbsty.throw() 
    }

    // Tolerances
    self.gradtol = 1e-6 // Convergence criterion based on norm of the gradient
    self.etol = 1e-8 // Convergence criterion based on change in value of objective function
    self.ctol = 1e-10 // Constraint tolerance
    self.maxconsiterations = 100 // Maximum number of iterations to satisfy constraints

    self._valuehistory=[] // History of objective function values 
  } 

  /* Convenience methods to calculate quantities */

  value() { // Value of the objective function
    var v = self.adapter.value()
    if (isinf(v) || isnan(v)) _OptInf.throw() 
    return v
  }

  constraintvalue() { // Value of constraint functions as a vector
    var a = super.constraintvalue()
    return Matrix(a)
  }

  constraintgradient() { // Gradient of constraint functions as a matrix
    var g = super.constraintgradient()
    return Matrix([g])
  }

  /* Verbosity */

  checkverbosity(level) { // Checks if output should be printed at a given level
    return (self.verbosity>=level)
  }

  warning(error) { // Generate a warning 
    if (self.checkverbosity(_OptQuiet)) error.warning() 
  }

  /* Convergence check */

  hasconverged() {
    // Convergence in value  |delta e|/|e| < etol or |delta e| < etol if e is < etol
    if (self._valuehistory && self._valuehistory.count()>1) {
      var f1=self._valuehistory[-1], f2=self._valuehistory[-2]

      // Compute relative change in value, or use absolute value if close to zero
      var de = abs(f1-f2), ee=(abs(f1)+abs(f2))/2, tol = self.etol 
      if (abs(f1)>tol) tol*=ee 
      if (de < tol) return true 
    } 

    return false 
  }

  /* Reporting */

  reportoptcond() { // Report value of optimality condition
    var g = self.gradient()
    if (!g) return ""
    return "grad=|${g.norm()}|"
  }

  report(iter) { // Reporting
    if (!self.checkverbosity(_OptNormal)) return 

    var f = self.value() 

    print "Iteration ${iter.format("%2i")}: ${f.format("%12g")} ${self.reportoptcond()}" 
  }

  /* Optimization sequence */

  start() { }           // Initialization at the beginning of an optimization
  begin() { }           // Calculate initial information for the iteration
  step() { }            // Perform optimization step 
  next() { }            // Calculate updated information

  record() {            // Record information about the iteration
    self._valuehistory.append(self.value())
  }

  iterate() {  // Optimization iteration
    self.begin()  
    self.step()
    self.next()
  }

  optimize(nsteps) { // Optimization loop
    self.start()
    self.record() 
    for (i in 1..nsteps) {
      if (self.hasconverged()) return 
      self.iterate() 
      self.report(i)
      self.record()
    }

    self.warning(_OptMaxErr)
  }
}

/* -----------------------------------------
 * GradientDescentController
 * 
 * Simple gradient descent at fixed stepsize 
 * ----------------------------------------- */ 

class GradientDescentController is OptimizationController {
  init(adapter, quiet=false, verbosity=nil, stepsize=0.1) { 
    super.init(adapter, quiet=quiet, verbosity=verbosity)
    self.stepsize=stepsize
    self._direction=nil 
  }

  // Define a descent direction
  setdirection(d) { self._direction = d }
  direction() { return self._direction}

  searchdirection() { // Determine the search direction
    self.setdirection(-self.gradient())
  }

  begin() { self.searchdirection() }

  step() { // Take a step in the current search direction
    var x = self.get() 
    x+=self.stepsize*self.direction()
    self.set(x)
  }
}

/* -----------------------------------------
 * LineSearchController
 * 
 * Gradient descent with linesearches
 * ----------------------------------------- */ 

class LineSearchController is GradientDescentController {
  init(adapter, quiet=false, verbosity=nil, stepsize=0.1, alpha=0.2, beta=0.5) {
    super.init(adapter, quiet=quiet, verbosity=verbosity, stepsize=stepsize)
    self.maxsteps = 50 // Maximum steps for reducing stepsize
    self.alpha = alpha // } Coefficients 
    self.beta = beta   // }
  }

  expecteddescent() { // Predict expected descent
    var d = self.direction()
    var g = self.gradient() 

    self.df = g.inner(d)

    if (self.df>0) _OptLnSrchDrn.warning()
  }

  accept(t) { // Have we descended sufficiently?
    return self.value() < self._ovalue + self.alpha*t*self.df
  }

  stepwith(x0, t) { // Take a step of size t from x0 in the descent direction
    self.set(x0 + t*self.direction())
    return self.value()
  }

  step() {// Perform a single linesearch
    self.expecteddescent()
    var t=1
    var success=false
    self._ovalue = self.value() // Save value for comparison

    var x0 = self.get() 
    for (nsteps in 1..self.maxsteps) {
        self.stepwith(x0, t)
        success=self.accept(t)
        if (success) break
        t*=self.beta
    }
    if (!success) _OptLnSrchStpsz.warning()

    self.stepsize=t
  }
}
