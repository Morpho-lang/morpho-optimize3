/* ************************************************************
 * OptimizationAdapter 
 * ************************************************************ */ 

/* --------------------------
 * Errors
 * -------------------------- */ 

var _NoActiveFunctionalsErr = Error("NoActiveFunctionals", "Problem has no active functionals.")

/* --------------------------
 * OptimizationAdapter class
 * -------------------------- */ 

/* OptimizationAdapters provide a common interface that enables optimization algorithms to obtain 
   necessary information about the an optimization problem. */

class OptimizationAdapter {
  init(target) { self.target = target }

  set(x) { return nil }               // Sets the parameters 
  get() { return 0 }                  // Returns current value of the parameters

  value() { return 0 }                // Returns the current value of the objective function at the current parameters
  gradient() { return nil }           // Returns the gradient of the objective function at the current parameters as a column vector
  hessian() { return nil }            // (Optional) Returns the hessian of the objective function at the current parameters as a matrix

  constraintvalue() { return nil }    // Returns the value(s) of any constraints as a column vector
  constraintgradient() { return nil } // Returns the gradient(s) of any constraints as a matrix
}

/* -------------------------
 * FunctionAdapter 
 * ------------------------- */

class FunctionAdapter is OptimizationAdapter {
  init(target, gradient=nil, start=nil, constraints=nil, constraintgradients=nil) {
    super.init(target)
    self.x = start
    if (gradient) self.gradfn = gradient 
    else self.gradfn = self._numericalgrad
  }

  _listify(x) { // Converts an enumerable to a list
    var l = []
    for (e in x) l.append(e)
    return l 
  }

  _numericalgrad(...u) { // Numerical gradient
    @
    //print u
    //var grad = Matrix(x.count())
    return nil 
  }

  set(x) { self.x = x }
  get() { return self.x }

  value() { return apply(self.target, self._listify(self.x)) }
  gradient() { return apply(self.gradfn, self._listify(self.x)) }

  constraintvalue() { return nil }   
  constraintgradient() { return nil }
}
