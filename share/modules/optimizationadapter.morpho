/* ************************************************************
 * OptimizationAdapter 
 * ************************************************************ */ 

/* --------------------------
 * Errors
 * -------------------------- */ 

var _NoActiveFunctionalsErr = Error("NoActiveFunctionals", "Problem has no active functionals.")

/* --------------------------
 * OptimizationAdapter class
 * -------------------------- */ 

/* OptimizationAdapters provide a common interface that enables optimization algorithms to obtain 
   necessary information about the an optimization problem. */

class OptimizationAdapter {
  init(target) { self.target = target }

  set(x) { return nil }               // Sets the parameters 
  get() { return 0 }                  // Returns current value of the parameters

  value() { return 0 }                // Returns the current value of the objective function at the current parameters
  gradient() { return nil }           // Returns the gradient of the objective function at the current parameters as a column vector
  hessian() { return nil }            // (Optional) Returns the hessian of the objective function at the current parameters as a matrix

  countconstraints() { return 0 }     // Returns number of constraints present
  constraintvalue() { return nil }    // Returns the value(s) of any constraints as a column vector
  constraintgradient() { return nil } // Returns the gradient(s) of any constraints as a matrix
}

/* -------------------------
 * FunctionAdapter 
 * ------------------------- */

/* Adapts a provided function (and an optional gradient) to the OptimizationAdapter interface */

var _eps=(1e-16)^(1/3) // Optimal relative stepsize for cell centered differences

class FunctionAdapter is OptimizationAdapter {
  init(target, gradient=nil, start=nil, constraints=nil, constraintgradients=nil) {
    super.init(target)
    self.x = start
    
    self.gradfn = gradient 
    if (!gradient) self.gradfn = self._numericalgrad(self.target) // Use FD if no grad provided

    self.constraints = constraints
    self.constraintgradfn = constraintgradients
    if (self.constraints && !self.constraintgradfn) {
      var lst = []
      for (c in self.constraints) lst.append(self._numericalgrad(c))
      self.constraintgradfn = lst 
    }
  }

  _listify(x) { // Converts an enumerable to a list
    var l = []
    for (e in x) l.append(e)
    return l 
  }

  _stepsize(x) {
    var h=_eps
    var xx=abs(x)
    if (xx>1) h*=xx // Scale

    var temp = x+h  // Ensure we obtain an FP representable number
    return temp - x 
  }

  _numericalgrad(f) { // Construct a numerical gradient as a closure 
    fn gradfn(...x) {
      var n = x.count()
      var grad = Matrix(n)

      for (var i=0; i<n; i+=1) {
        var x0 = x[i]
        var eps = self._stepsize(x0)
        x[i]=x0+eps
        var fr = apply(f, x)
        x[i]=x0-eps
        var fl = apply(f, x)
        grad[i]=(fr-fl)/(2*eps)
      }

      return grad 
    }
    return gradfn
  }

  set(x) { self.x = x }
  get() { return self.x }

  value() { return apply(self.target, self._listify(self.x)) }
  gradient() { return apply(self.gradfn, self._listify(self.x)) }

  countconstraints() {
    if (self.constraints) return self.constraints.count()
    return 0
  }

  constraintvalue() { 
    if (!self.constraints) return nil 

    var val = []
    for (c in self.constraints) val.append(apply(c, self._listify(self.x)))
    return val
  }   

  constraintgradient() { 
    if (!self.constraints) return nil 

    var val = []
    for (c in self.constraintgradfn) val.append( apply(c, self._listify(self.x)) )
    return val
  }
}
